{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864e4469-cd3c-44c8-8dc0-cfcb9c8253d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "with open(\"../xfdata/dataset/job_list.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    job_list = json.load(f)\n",
    "with open(\"../xfdata/dataset/train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train = json.load(f)\n",
    "with open(\"../xfdata/dataset/test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test = json.load(f)\n",
    "    \n",
    "# 1ã€æ•™è‚²ç»å†\n",
    "with open(\"../user_data/profileEduExps_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    profileEduExps_sentences = json.load(f)\n",
    "\n",
    "# 2ã€ç¤¾ä¼šç»å†\n",
    "with open(\"../user_data/profileSocialExps_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    profileSocialExps_sentences = json.load(f)\n",
    "\n",
    "# 3ã€é¡¹ç›®ç»å†\n",
    "with open(\"../user_data/profileProjectExps_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    profileProjectExps_sentences = json.load(f)\n",
    "\n",
    "# 4ã€å·¥ä½œç»å†\n",
    "with open(\"../user_data/profileWorkExps_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    profileWorkExps_sentences = json.load(f)\n",
    "\n",
    "# 5ã€æŠ€èƒ½\n",
    "with open(\"../user_data/profileSkills_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    profileSkills_sentences = json.load(f)\n",
    "\n",
    "# 6ã€è£èª‰\n",
    "with open(\"../user_data/profileAwards_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    profileAwards_sentences = json.load(f)\n",
    "\n",
    "# 7ã€æ±‚èŒæ„æ„¿\n",
    "with open(\"../user_data/profileDesire_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    profileDesire_sentences = json.load(f)\n",
    "\n",
    "# 8ã€è¯­è¨€\n",
    "with open(\"../user_data/profileLanguage_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    profileLanguage_sentences = json.load(f)\n",
    "\n",
    "# èŒä½\n",
    "with open(\"../user_data/job_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    job_sentences = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff3db73-6a23-4b49-ad83-a6de7523dce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# è·å–æ ‡ç­¾\n",
    "labels = [i[\"positionID\"] for i in train]\n",
    "labelencoder = LabelEncoder()\n",
    "labels = labelencoder.fit_transform(labels)\n",
    "print(len(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ff9dd2-4589-4a20-a327-bb747b09cd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 6500\n",
      "16000 16000\n",
      "4000 4000\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "# è°ƒæ•´ä½ç½®\n",
    "for s1, s2, s3, s4, s5, s6, s7, s8 in zip(profileEduExps_sentences, profileWorkExps_sentences, profileProjectExps_sentences, \\\n",
    "profileSocialExps_sentences, profileSkills_sentences, profileAwards_sentences, profileDesire_sentences, \\\n",
    "profileLanguage_sentences):\n",
    "    sentences.append(s1 + s2 + s3 + s4 + s5 + s6 + s7 + s8)\n",
    "\n",
    "train_sentences, test_sentences = sentences[: len(train)], sentences[len(train): ]\n",
    "print(len(train_sentences), len(test_sentences))\n",
    "train_idx = np.load(\"../user_data/train_idx.npy\")\n",
    "valid_idx = np.load(\"../user_data/valid_idx.npy\")\n",
    "train_sentences, valid_sentences = [train_sentences[idx] for idx in train_idx], [train_sentences[idx] for idx in valid_idx]\n",
    "train_labels, valid_labels = [labels[idx] for idx in train_idx], [labels[idx] for idx in valid_idx]\n",
    "print(len(train_sentences), len(train_labels))\n",
    "print(len(valid_sentences), len(valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b17536-0621-4c9c-85b2-524de030f06d",
   "metadata": {},
   "source": [
    "# NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0233243f-4ac9-4936-b742-82b49908996d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForNextSentencePrediction were not initialized from the model checkpoint at ../user_data/output/checkpoint-132800 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForNextSentencePrediction, BertTokenizer, BertConfig\n",
    "\n",
    "# åŠ è½½ç°æœ‰çš„MLMé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„\n",
    "path = \"../user_data/output/checkpoint-132800\"\n",
    "# åŠ è½½é…ç½®\n",
    "config = BertConfig.from_pretrained(path)\n",
    "# å®ä¾‹åŒ– NSP ä»»åŠ¡çš„æ¨¡å‹\n",
    "model = BertForNextSentencePrediction.from_pretrained(path, config=config)\n",
    "# åŠ è½½åŒæ ·çš„ tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab9dddc-09ec-44b8-a9ef-bb9b7777168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "dataset_for_nsp = load_from_disk(\"../user_data/dataset_for_nsp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c9b456-750d-43d8-9d10-cfa30e424522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [03:23<00:00, 78.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# è·å–æ¯è¡Œä¸­æœ€å¤§çš„ä¸‰ä¸ªå€¼åŠå…¶ç´¢å¼•\n",
    "topk_values, topk_indices = torch.topk(dataset_for_nsp[\"train\"][\"logits\"], k=10, dim=1)\n",
    "# é€šè¿‡å¾ªç¯æ„å»ºæ•°æ®é›†\n",
    "input_ids_train = []\n",
    "token_type_ids_train = []\n",
    "atttention_mask_train = []\n",
    "labels_train = []\n",
    "for i in tqdm(range(16000)):\n",
    "    topk_indice = topk_indices[i]\n",
    "    label = train_labels[i]\n",
    "    for j in set(topk_indice.tolist() + [label]):\n",
    "        resume = train_sentences[i]\n",
    "        job = job_sentences[j]\n",
    "        input_ids, token_type_ids, attention_mask = tokenizer.encode_plus(\n",
    "            resume, \n",
    "            job,\n",
    "            padding=\"max_length\",\n",
    "            max_length=1024,\n",
    "            truncation=\"only_first\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).values()\n",
    "        input_ids_train.append(input_ids)\n",
    "        token_type_ids_train.append(token_type_ids)\n",
    "        atttention_mask_train.append(attention_mask)\n",
    "        if j == label:\n",
    "            labels_train.append(1)\n",
    "        else:\n",
    "            labels_train.append(0)\n",
    "\n",
    "input_ids_train = torch.cat(input_ids_train)\n",
    "token_type_ids_train = torch.cat(token_type_ids_train)\n",
    "atttention_mask_train = torch.cat(atttention_mask_train)\n",
    "\n",
    "train_encoded_nsp = Dataset.from_dict({\n",
    "    \"input_ids\": input_ids_train,\n",
    "    \"token_type_ids\": token_type_ids_train,\n",
    "    \"attention_mask\": atttention_mask_train,\n",
    "    \"labels\": labels_train\n",
    "})\n",
    "\n",
    "train_encoded_nsp.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb34a97b-2dfc-4a31-97ae-fb0fba57c8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:49<00:00, 80.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# è·å–æ¯è¡Œä¸­æœ€å¤§çš„ä¸‰ä¸ªå€¼åŠå…¶ç´¢å¼•\n",
    "topk_values, topk_indices = torch.topk(dataset_for_nsp[\"valid\"][\"logits\"], k=10, dim=1)\n",
    "# é€šè¿‡å¾ªç¯æ„å»ºæ•°æ®é›†\n",
    "input_ids_valid = []\n",
    "token_type_ids_valid = []\n",
    "atttention_mask_valid = []\n",
    "labels_valid = []\n",
    "for i in tqdm(range(4000)):\n",
    "    topk_indice = topk_indices[i]\n",
    "    label = valid_labels[i]\n",
    "    for j in set(topk_indice.tolist() + [label]):\n",
    "        resume = valid_sentences[i]\n",
    "        job = job_sentences[j]\n",
    "        input_ids, token_type_ids, attention_mask = tokenizer.encode_plus(\n",
    "            resume, \n",
    "            job,\n",
    "            padding=\"max_length\",\n",
    "            max_length=1024,\n",
    "            truncation=\"only_first\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).values()\n",
    "        input_ids_valid.append(input_ids)\n",
    "        token_type_ids_valid.append(token_type_ids)\n",
    "        atttention_mask_valid.append(attention_mask)\n",
    "        if j == label:\n",
    "            labels_valid.append(1)\n",
    "        else:\n",
    "            labels_valid.append(0)\n",
    "\n",
    "input_ids_valid = torch.cat(input_ids_valid)\n",
    "token_type_ids_valid = torch.cat(token_type_ids_valid)\n",
    "atttention_mask_valid = torch.cat(atttention_mask_valid)\n",
    "\n",
    "valid_encoded_nsp = Dataset.from_dict({\n",
    "    \"input_ids\": input_ids_valid,\n",
    "    \"token_type_ids\": token_type_ids_valid,\n",
    "    \"attention_mask\": atttention_mask_valid,\n",
    "    \"labels\": labels_valid\n",
    "})\n",
    "\n",
    "valid_encoded_nsp.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6952ece-b3c3-4892-878d-6f5ca05a14f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6500/6500 [01:21<00:00, 79.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# è·å–æ¯è¡Œä¸­æœ€å¤§çš„ä¸‰ä¸ªå€¼åŠå…¶ç´¢å¼•\n",
    "topk_values, topk_indices = torch.topk(dataset_for_nsp[\"test\"][\"logits\"], k=10, dim=1)\n",
    "# é€šè¿‡å¾ªç¯æ„å»ºæ•°æ®é›†\n",
    "input_ids_test = []\n",
    "token_type_ids_test = []\n",
    "atttention_mask_test = []\n",
    "for i in tqdm(range(6500)):\n",
    "    topk_indice = topk_indices[i]\n",
    "    for j in set(topk_indice.tolist()):\n",
    "        resume = test_sentences[i]\n",
    "        job = job_sentences[j]\n",
    "        input_ids, token_type_ids, attention_mask = tokenizer.encode_plus(\n",
    "            resume, \n",
    "            job,\n",
    "            padding=\"max_length\",\n",
    "            max_length=1024,\n",
    "            truncation=\"only_first\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).values()\n",
    "        input_ids_test.append(input_ids)\n",
    "        token_type_ids_test.append(token_type_ids)\n",
    "        atttention_mask_test.append(attention_mask)\n",
    "        \n",
    "input_ids_test = torch.cat(input_ids_test)\n",
    "token_type_ids_test = torch.cat(token_type_ids_test)\n",
    "atttention_mask_test = torch.cat(atttention_mask_test)\n",
    "\n",
    "test_encoded_nsp = Dataset.from_dict({\n",
    "    \"input_ids\": input_ids_test,\n",
    "    \"token_type_ids\": token_type_ids_test,\n",
    "    \"attention_mask\": atttention_mask_test,\n",
    "})\n",
    "\n",
    "test_encoded_nsp.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c50e7f4e-748f-4300-a497-0dd2f10753af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 19:57:02.432178: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-16 19:57:02.447592: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-16 19:57:02.452238: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-16 19:57:02.465562: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 19:57:03.098533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../user_data/nsp_output\",\n",
    "    evaluation_strategy=\"epoch\",  # æ¯ä¸ªepochåè¿›è¡ŒéªŒè¯\n",
    "    per_device_train_batch_size=64,  # å¯ä»¥æ ¹æ®æ˜¾å­˜è°ƒæ•´\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,  # å¾®è°ƒçš„è½®æ•°\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='../user_data/nsp_log',  # ä¿å­˜æ—¥å¿—æ–‡ä»¶çš„ä½ç½®\n",
    "    save_strategy=\"epoch\",  # æ¯ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91492fc4-2227-440a-bb5a-44b01f799ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "import torch\n",
    "\n",
    "\n",
    "# è®¡ç®—ç±»åˆ«æƒé‡\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_encoded_nsp[\"labels\"].numpy()), y=train_encoded_nsp[\"labels\"].numpy())\n",
    "# è½¬æ¢ä¸ºtensoræ ¼å¼ä»¥ä¾¿åç»­ä½¿ç”¨\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\")\n",
    "\n",
    "# å®šä¹‰å¸¦æƒé‡çš„æŸå¤±å‡½æ•°\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# è‡ªå®šä¹‰Trainer\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\").to(\"cuda\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe703098-2bb3-4083-a740-214fcd0e279f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='25110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   48/25110 00:34 < 5:13:54, 1.33 it/s, Epoch 0.02/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_encoded_nsp,\n",
    "    eval_dataset=valid_encoded_nsp,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c3dec06-9917-4cf4-aaed-6101cefdd544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:49<00:00, 81.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# è·å–æ¯è¡Œä¸­æœ€å¤§çš„ä¸‰ä¸ªå€¼åŠå…¶ç´¢å¼•\n",
    "topk_values, topk_indices = torch.topk(dataset_for_nsp[\"valid\"][\"logits\"], k=10, dim=1)\n",
    "# é€šè¿‡å¾ªç¯æ„å»ºæ•°æ®é›†\n",
    "input_ids_valid = []\n",
    "token_type_ids_valid = []\n",
    "atttention_mask_valid = []\n",
    "labels_valid = []\n",
    "for i in tqdm(range(4000)):\n",
    "    topk_indice = topk_indices[i]\n",
    "    label = valid_labels[i]\n",
    "    for j in set(topk_indice.tolist()):\n",
    "        resume = valid_sentences[i]\n",
    "        job = job_sentences[j]\n",
    "        input_ids, token_type_ids, attention_mask = tokenizer.encode_plus(\n",
    "            resume, \n",
    "            job,\n",
    "            padding=\"max_length\",\n",
    "            max_length=1024,\n",
    "            truncation=\"only_first\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).values()\n",
    "        input_ids_valid.append(input_ids)\n",
    "        token_type_ids_valid.append(token_type_ids)\n",
    "        atttention_mask_valid.append(attention_mask)\n",
    "        if j == label:\n",
    "            labels_valid.append(1)\n",
    "        else:\n",
    "            labels_valid.append(0)\n",
    "\n",
    "input_ids_valid = torch.cat(input_ids_valid)\n",
    "token_type_ids_valid = torch.cat(token_type_ids_valid)\n",
    "atttention_mask_valid = torch.cat(atttention_mask_valid)\n",
    "\n",
    "valid_encoded_nsp = Dataset.from_dict({\n",
    "    \"input_ids\": input_ids_valid,\n",
    "    \"token_type_ids\": token_type_ids_valid,\n",
    "    \"attention_mask\": atttention_mask_valid,\n",
    "    \"labels\": labels_valid\n",
    "})\n",
    "\n",
    "valid_encoded_nsp.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "result_valid = trainer.predict(valid_encoded_nsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ac77eb7-e1bb-4179-b403-759d32f5ee4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37697416956310054"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# è·å–æ¯è¡Œä¸­æœ€å¤§çš„ä¸‰ä¸ªå€¼åŠå…¶ç´¢å¼•\n",
    "topk_values, topk_indices = torch.topk(dataset_for_nsp[\"valid\"][\"logits\"], k=10, dim=1)\n",
    "start = 0\n",
    "valid_logits = result_valid.predictions[:, 1]\n",
    "pred_valid = []\n",
    "count = 0\n",
    "for i in range(4000):\n",
    "    topk_indice = topk_indices[i]\n",
    "    label = valid_labels[i]\n",
    "    choices = list(set(topk_indice.tolist()))\n",
    "    logits_choices = valid_logits[: len(choices)]\n",
    "    if nn.Softmax(dim=1)(topk_values[[i]])[0][0] < 0.25:\n",
    "        count += 1\n",
    "        choice = choices[logits_choices.argmax()]\n",
    "    else: \n",
    "        choice = topk_indice[0]\n",
    "    pred_valid.append(choice)\n",
    "    valid_logits = valid_logits[len(choices): ]\n",
    "print(count)\n",
    "f1_score(y_pred=pred_valid, y_true=valid_labels, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686726c-28e1-47cb-9c55-eb3637bae7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.375266757877344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "ebb70f0b-653c-430d-acae-19027a7be5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = trainer.predict(test_encoded_nsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a78b600c-c112-422b-972a-3beebe6f8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å–æ¯è¡Œä¸­æœ€å¤§çš„ä¸‰ä¸ªå€¼åŠå…¶ç´¢å¼•\n",
    "topk_values, topk_indices = torch.topk(dataset_nsp[\"test\"][\"logits\"], k=3, dim=1)\n",
    "start = 0\n",
    "test_logits = result.predictions[:, 1]\n",
    "pred_test = []\n",
    "for i in range(6500):\n",
    "    topk_indice = topk_indices[i]\n",
    "    choices = topk_indice\n",
    "    logits_choices = test_logits[: len(choices)]\n",
    "    if nn.Softmax(dim=1)(topk_values[[i]])[0][0] < 0.6:\n",
    "        choice = choices[logits_choices.argmax()]\n",
    "    else: \n",
    "        choice = topk_indice[0]\n",
    "    pred_test.append(choice.item())\n",
    "    test_logits = test_logits[len(choices): ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "6e08a4d0-1b6e-4beb-9a56-a275752e0899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = pd.read_csv(\"../xfdata/example.csv\")\n",
    "df_submit[\"positionID\"] = labelencoder.inverse_transform(pred_test)\n",
    "df_submit.to_csv(\"../prediction_result/cls_nsp.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
