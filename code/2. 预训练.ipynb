{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2e49e5b-2eb6-4e44-85f0-a47515c7eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "# 8、语言\n",
    "with open(\"../user_data/corpus.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "083007ed-8644-4821-acc4-23fa7af3b615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4510\n",
      "Encoded: [1, 4439, 4438, 4461, 513, 4502, 4398, 4459, 280, 2470, 2942, 2781, 861, 2528, 624, 2477, 3391, 4489, 280, 2470, 2942, 3169, 2781, 2528, 624, 4458, 4501, 2965, 2600, 2362, 2477, 4437, 4499, 4460, 4386, 4388, 4462, 4438, 4461, 1246, 4502, 1829, 4459, 207, 314, 2942, 2781, 861, 2528, 624, 2477, 3391, 4489, 2528, 2477, 4425, 280, 2470, 2781, 861, 672, 4425, 3169, 2781, 2528, 624, 4458, 4501, 1631, 1627, 2362, 2477, 4437, 4499, 4460, 4369, 4359, 4462, 4463, 4443, 4466, 4441, 4451, 4485, 4497, 4457, 4498, 2213, 2884, 1121, 929, 619, 3189, 402, 2361, 1723, 2681, 1190, 1121, 929, 3347, 1050, 653, 755, 527, 616, 2942, 2443, 154, 4488, 4460, 4385, 4397, 4474, 4451, 4485, 4497, 4457, 4498, 2213, 2884, 4409, 4435, 20, 20, 30, 402, 1969, 1080, 3169, 4104, 653, 755, 2443, 154, 4488, 4460, 4388, 4474, 4451, 4485, 4497, 283, 2884, 207, 3711, 708, 4115, 2353, 2443, 1101, 4278, 2975, 546, 1950, 3056, 2281, 2876, 3376, 2870, 1022, 4053, 2940, 3056, 2465, 1307, 1893, 2057, 866, 819, 2719, 278, 402, 1131, 1780, 3056, 3721, 207, 3711, 708, 4115, 2353, 402, 4404, 4424, 4419, 1204, 1190, 1892, 2943, 2622, 399, 2181, 3056, 2841, 755, 402, 866, 819, 4306, 2876, 1354, 1531, 1103, 1571, 2649, 3376, 2334, 1107, 3056, 2975, 2882, 4009, 1804, 4262, 2941, 3376, 3474, 3056, 890, 1781, 2882, 2213, 2884, 2274, 467, 746, 708, 758, 402, 2823, 278, 4401, 4404, 4400, 2814, 2975, 211, 2645, 402, 1354, 1531, 1103, 1571, 2980, 32, 2603, 2885, 4261, 2056, 1084, 2876, 2965, 402, 1779, 1121, 365, 2814, 1313, 283, 1137, 2814, 522, 744, 1050, 402, 1901, 2470, 3056, 2612, 4079, 2430, 1833, 4261, 4049, 1779, 1121, 2542, 1823, 3056, 2603, 1779, 1121, 2542, 1823, 2814, 2975, 4401, 4404, 4400, 402, 3811, 108, 211, 2645, 3056, 2738, 2823, 278, 4400, 4409, 4409, 1937, 890, 1894, 752, 1190, 2781, 861, 3056, 3169, 4248, 776, 2184, 3717, 3063, 4401, 4404, 4400, 402, 752, 1190, 2781, 861, 2142, 4089, 2920, 2781, 861, 2430, 1248, 330, 4306, 2876, 4261, 4049, 1354, 1531, 3056, 2049, 1098, 1833, 1313, 2195, 2975, 866, 819, 4306, 2876, 2980, 32, 2823, 278, 4416, 4420, 4410, 4412, 1890, 4414, 8, 18, 8, 21, 6, 22, 15, 24, 20, 1190, 2711, 2882, 2941, 2967, 3478, 3324, 3056, 2870, 2794, 278, 1057, 2945, 2841, 2643, 1354, 1531, 1084, 2876, 3056, 1232, 2010, 2287, 593, 4256, 4049, 1354, 1531, 2872, 522, 2738, 2264, 2841, 1354, 1531, 752, 1197, 3056, 2181, 1080, 4113, 2663, 2664, 2023, 752, 1197, 3613, 4410, 30, 21, 4423, 4412, 4457, 4498, 2213, 2884, 4400, 4409, 4409, 402, 2681, 1190, 207, 3711, 708, 866, 819, 4306, 2876, 402, 1354, 1531, 653, 755, 4488, 4460, 4385, 4474, 4464, 4442, 4454, 4485, 3423, 2166, 4484, 4183, 4190, 1943, 2014, 2354, 314, 2274, 2931, 3423, 402, 1072, 2768, 2886, 2010, 2870, 1890, 2274, 2931, 3423, 551, 314, 560, 2528, 2814, 2980, 32, 3755, 2650, 2274, 2931, 3423, 3006, 2837, 430, 3423, 2166, 3005, 3771, 4123, 4509, 1631, 1627, 2362, 2477, 1414, 2274, 2931, 3423, 2477, 270, 3423, 2166, 3711, 2184, 2835, 4507, 4508, 4460, 4388, 4386, 4476, 4465, 4450, 4453, 723, 624, 2213, 1245, 2004, 4453, 4414, 8, 18, 8, 4453, 4404, 4453, 4424, 30, 20, 16, 9, 10, 4089, 4128, 4453, 2179, 593, 2682, 1896, 4453, 15, 8, 18, 8, 2811, 653, 4453, 4410, 30, 4421, 23, 12, 4453, 4422, 4435, 4436, 15, 21, 4453, 4261, 624, 3189, 3169, 2781, 1890, 1833, 399, 1836, 723, 624, 4473, 4455, 4477, 4444, 4446, 4469, 4452, 4487, 1291, 3169, 4490, 1414, 2946, 560, 2340, 2477, 3069, 4475, 4445, 4468, 4444, 4505, 1414, 479, 2835, 2528, 2814, 442, 1191, 2036, 2470, 2340, 4467, 4447, 4482, 4483, 4491, 4500, 4472, 4470, 4440, 4449, 4486, 3809, 4089, 4495, 1528, 257, 365, 803, 3809, 1084, 1216, 1093, 1072, 2768, 3809, 4089, 2900, 1528, 3677, 2007, 4448, 4471, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-09-16 21:19:48.112680: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-16 21:19:48.134399: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-16 21:19:48.141170: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-16 21:19:48.159235: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 21:19:49.587632: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded: [CLS] <begin_EduExps> <begin_EduExp> <education> 14722706&14853292 <schoolLevel> 62&63&50&53&54&54 <department> 14717874 14854807 14925705 14923942 14780072 14856354 14725004 14854817 15638181 <major> 14717874 14854807 14925705 15632285 14923942 14856354 14725004 <courses> <school> 14925738 14857910 14853024 14854817 <GPA> <ranking> <duration> 53 54 <end_EduExp> <begin_EduExp> <education> 14785451&14721174 <schoolLevel> 14844587&14847385&15641759&14788518 <department> 14716590 14718849 14925705 14923942 14780072 14856354 14725004 14854817 15638181 <major> 14856354 14854817 88 14717874 14854807 14923942 14780072 14726844 88 15632285 14923942 14856354 14725004 <courses> <school> 14792344 14792334 14853024 14854817 <GPA> <ranking> <duration> 51 50 <end_EduExp> <end_EduExps> <begin_WorkExps> <end_WorkExps> <begin_ProjectExps> <begin_projectExp> <jobTitle> <projectDesc> <companyName> <projectName> 14850237 14925193 14783159 14781097 14724797 15634611 14720387 14853021 14794135 14859141 14784899 14783159 14781097 15637633 14782359 14726332 14728344 14722963 14724785 14925705 14854553 14715319 <location> <duration> 52 62 <end_projectExp> <begin_projectExp> <jobTitle> <projectDesc> <companyName> <projectName> 14850237 14925193 73 98 115 115 126 14720387 14846644 14782641 15632285 15706536 14726332 14728344 14854553 14715319 <location> <duration> 54 <end_projectExp> <begin_projectExp> <jobTitle> <projectDesc> 14717878 14925193 14716590 15697555 14727830 15706553 14852992 14854553 14782906 15710596 14925757 14723463 14846610 15252363 14851261 14924977 15637945 14924962 14782100 15705762 14925702 15252363 14854590 14786452 14846096 14847927 14780081 14779272 14859667 14717871 14720387 14783420 14844092 15252363 15697803 14716590 15697555 14727830 15706553 14852992 14720387 68 87 82 14784945 14784899 14846095 14925706 14858170 14720191 14849931 15252363 14924711 14728344 14720387 14780081 14779272 15710888 14924977 14787719 14791308 14782910 14791826 14858639 15637945 14852744 14783119 15252363 14925757 14925185 15704228 14844340 15710366 14925703 15637945 15639967 15252363 14780567 14844093 14925185 14850237 14925193 14851257 14721689 14728328 14727830 14728347 14720387 14924472 14717871 65 68 64 14924443 14925757 14716606 14858630 14720387 14787719 14791308 14782910 14791826 14976901 13 14858113 14925195 15710364 14847921 14782848 14924977 14925738 14720387 14844090 14783159 14719884 14924443 14786472 14717878 14783626 14924443 14722950 14728323 14782359 14720387 14846114 14854807 15252363 14858138 15706258 14854334 14844593 15710364 15705739 14844090 14783159 14856843 14844578 15252363 14858113 14844090 14783159 14856843 14844578 14924443 14925757 65 68 64 14720387 15699333 14714302 14716606 14858630 15252363 14859953 14924472 14717871 64 73 73 14846396 14780567 14846097 14728340 14784899 14923942 14780072 15252363 15632285 15710336 14728379 14849935 15697796 15630472 65 68 64 14720387 14728340 14784899 14923942 14780072 14849419 15706282 14925454 14923942 14780072 14854334 14785453 14719138 15710888 14924977 15710364 15705739 14787719 14791308 15252363 14847907 14782890 14844593 14786472 14849953 14925757 14780081 14779272 15710888 14924977 14976901 13 14924472 14717871 79 83 74 75 14846093 77 102 113 102 116 100 117 110 119 115 14784899 14859453 14925185 14925703 14925741 15640242 15637157 15252363 14924962 14924188 14717871 14782384 14925709 14924711 14858626 14787719 14791308 14782848 14924977 15252363 14785418 14847398 14851759 14724264 15710359 15705739 14787719 14791308 14924964 14722950 14859953 14851225 14924711 14787719 14791308 14728340 14784923 15252363 14849931 14782641 15706551 14858898 14858900 14847626 14728340 14784923 15696052 74 126 116 86 75 <companyName> <projectName> 14850237 14925193 64 73 73 14720387 14859141 14784899 14716590 15697555 14727830 14780081 14779272 15710888 14924977 14720387 14787719 14791308 14726332 14728344 <location> <duration> 52 <end_projectExp> <end_ProjectExps> <begin_SocialExps> <begin_socialExp> <jobTitle> 15638913 14849695 <jobDescr> 15709080 15709092 14846600 14847406 14852995 14718849 14851257 14925473 15638913 14720387 14782626 14860223 14925196 14847398 14924962 14846093 14851257 14925473 15638913 14723494 14718849 14723726 14856354 14924443 14976901 13 15698352 14858640 14851257 14925473 15638913 15042459 14924703 14721159 15638913 14849695 15042458 15698596 15707534 organization 14792344 14792334 14853024 14854817 14788518 14851257 14925473 15638913 14854817 14717848 15638913 14849695 15697555 14849935 14924701 department location <duration> 54 53 <end_socialExp> <end_SocialExps> <begin_professionalSkills> <begin_skill> 14728081 14725004 14850237 14785451 14847384 <begin_skill> 77 102 113 102 <begin_skill> 68 <begin_skill> 87 126 115 111 104 105 15706282 15708039 <begin_skill> 14849929 14724264 14859143 14846102 <begin_skill> 110 102 113 102 14924436 14726332 <begin_skill> 74 126 84 118 107 <begin_skill> 85 98 99 110 116 <begin_skill> 15710364 14725004 15634611 15632285 14923942 14846093 14844593 14720191 14844596 14728081 14725004 <end_professionalSkills> <begin_softSkills> <end_softSkills> <begin_awards> <begin_contests> <end_contests> <begin_scholarship> <level> 14786217 15632285 <name> 14788518 14925710 14723726 14852753 14854817 15630486 <end_scholarship> <begin_certificates> <end_certificates> <begin_awards> award 14788518 14721977 14924701 14856354 14924443 14721192 14784902 14847873 14854807 14852753 <end_awards> <begin_desire> <function> <industry> <place> <salary> <end_place> <end_desire> <begin_Language> <begin_languages> <language> 15699126 15706282 <proficiency> 14791302 14717570 14719884 14778501 15699126 14782848 14785175 14782878 14782626 14860223 15699126 15706282 14925219 14791302 15697082 14847388 <begin_englishLevel> <end_englishLevel> [SEP]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "# 1. 创建词汇表（去重）\n",
    "all_tokens = set(token for sentence in corpus for token in sentence)\n",
    "vocab = {token: idx for idx, token in enumerate(sorted(all_tokens), start=5)}\n",
    "# 添加特殊 token (比如: [PAD], [CLS], [SEP], [UNK])\n",
    "special_tokens = {\"[PAD]\": 0, \"[CLS]\": 1, \"[SEP]\": 2, \"[UNK]\": 3, \"[MASK]\": 4}\n",
    "vocab = {**special_tokens, **vocab}\n",
    "print(len(vocab))\n",
    "\n",
    "# 2. 将词汇表保存为一个文件 (vocab.txt)\n",
    "with open(\"../user_data/vocab.txt\", \"w\", encoding=\"utf-8\") as vocab_file:\n",
    "    for token, idx in vocab.items():\n",
    "        vocab_file.write(f\"{token}\\n\")\n",
    "\n",
    "# 3. 加载词汇表并创建 BertTokenizer\n",
    "tokenizer = BertTokenizer(vocab_file=\"../user_data/vocab.txt\", do_lower_case=False)\n",
    "\n",
    "# 4. 测试 tokenizer\n",
    "sample_text = corpus[0]\n",
    "encoded = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {tokenizer.decode(encoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1736bfe-f578-4d7a-86f0-49c1f759fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, corpus, tokenizer, max_length=2048):\n",
    "        self.corpus = corpus\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.corpus[idx]\n",
    "\n",
    "        # 如果 tokens 长度超过 max_length - 2，随机选择一个起始位置\n",
    "        if len(tokens) > self.max_length - 2:\n",
    "            start_idx = random.randint(0, len(tokens) - (self.max_length - 2))\n",
    "            tokens = tokens[start_idx: start_idx + (self.max_length - 2)]\n",
    "        \n",
    "        # 进行编码处理，确保输入是适当长度\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tokens, \n",
    "            padding=\"max_length\",  # 如果不足 max_length，进行填充\n",
    "            trunction=True,\n",
    "            max_length=self.max_length,  # 保证长度为 max_length\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "        # 打印 input_ids 的最大值，查看是否超出 vocab_size\n",
    "        max_input_id = input_ids.max().item()\n",
    "        vocab_size = len(self.tokenizer.vocab)\n",
    "        \n",
    "        if max_input_id >= vocab_size:\n",
    "            print(f\"Error: input_id {max_input_id} exceeds vocab_size {vocab_size}\")\n",
    "            raise ValueError(f\"input_id {max_input_id} exceeds vocab_size {vocab_size}\")\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "# 假设你的 tokenizer 和 corpus 已经定义好\n",
    "dataset = CustomDataset(corpus, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df49ee-fb59-4e43-9347-9dbab8984e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22586' max='166000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22586/166000 1:39:08 < 10:29:33, 3.80 it/s, Epoch 13.61/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>5.583100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>5.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4980</td>\n",
       "      <td>4.669500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6640</td>\n",
       "      <td>3.587500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>2.846500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9960</td>\n",
       "      <td>2.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11620</td>\n",
       "      <td>2.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13280</td>\n",
       "      <td>2.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14940</td>\n",
       "      <td>1.860900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>1.737300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18260</td>\n",
       "      <td>1.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19920</td>\n",
       "      <td>1.558800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21580</td>\n",
       "      <td>1.492000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# 1. 定义 BERT 模型配置\n",
    "config = BertConfig(\n",
    "    vocab_size=len(tokenizer.vocab),  # 词汇表大小\n",
    "    max_position_embeddings=dataset.max_length,      # 最大序列长度\n",
    "    hidden_size=768,                  # 隐藏层大小\n",
    "    num_attention_heads=12,           # 注意力头的数量\n",
    "    num_hidden_layers=6,              # Transformer 层的数量\n",
    "    type_vocab_size=2,                # token 类型词表大小\n",
    "    pad_token_id=tokenizer.pad_token_id  # PAD token ID\n",
    ")\n",
    "\n",
    "\n",
    "# 2. 初始化一个从零开始的 BERT 模型\n",
    "model = BertForMaskedLM(config=config)\n",
    "\n",
    "# 3. 使用 DataCollatorForLanguageModeling 来自动遮盖 token\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # 遮盖 15% 的 token\n",
    ")\n",
    "\n",
    "# 4. 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../user_data/output\",       # 输出目录\n",
    "    overwrite_output_dir=True,   # 是否覆盖输出目录\n",
    "    num_train_epochs=100,          # 训练轮数\n",
    "    per_device_train_batch_size=16,  # 每个设备的批次大小\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",          # 每个epoch步保存模型\n",
    "    save_total_limit=2,          # 保留最近的 2 个保存点\n",
    "    logging_dir='../user_data/logs',        # 日志保存路径\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# 5. 创建 Trainer 实例\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,  # 你的自定义数据集\n",
    "    tokenizer=tokenizer     # 你的自定义 tokenizer\n",
    ")\n",
    "\n",
    "# 6. 开始训练\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
